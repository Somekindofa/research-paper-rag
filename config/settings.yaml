# Research RAG System Configuration
# ===================================

# LM Studio Server Configuration (Remote via Tailscale)
lm_studio:
  base_url: "http://100.0.0.0:1234/v1"  # Placeholder - update with actual Tailscale IP
  api_key: "not-needed"  # Override in .env if LM Studio requires API key
  model: "local-model"  # Default model - will be overridden by user selection
  temperature: 0.7
  max_tokens: 2048
  timeout: 120  # seconds

# Legacy Jan configuration (for backward compatibility)
jan:
  base_url: "http://100.0.0.0:1234/v1"
  api_key: "not-needed"
  model: "local-model"
  temperature: 0.7
  max_tokens: 2048
  timeout: 120

# Embedding Model Configuration
embeddings:
  model_name: "nomic-ai/nomic-embed-text-v1.5"
  device: "cuda"  # Use "cpu" if GPU issues
  batch_size: 32

# Chroma Vector Store Configuration
chroma:
  persist_directory: "data/chroma_db"
  collection_name: "research_papers"

# PDF Processing Configuration
pdf:
  folder_path: "data/pdfs"  # Override in .env
  scan_depth: 2  # Subfolders to scan (2 for Zotero storage structure)
  supported_extensions: [".pdf"]

# Chunking Configuration
chunking:
  chunk_size: 800
  chunk_overlap: 140
  separators:
    - "\n\n"
    - "\n"
    - ". "
    - " "

# Retrieval Configuration (MMR)
retrieval:
  k: 5  # Default number of documents to return (user configurable)
  fetch_k: 20  # Initial pool for MMR diversity
  lambda_mult: 0.7  # 0=max diversity, 1=max relevance
  score_threshold: 0.75  # Default relevance threshold (user configurable)

# Reranker Configuration
reranker:
  model_name: "cross-encoder/ms-marco-MiniLM-L-6-v2"
  top_k: 5  # Return top 5 after reranking

# Chainlit UI Configuration
ui:
  title: "Research RAG Assistant"
  description: "Query your research paper library with AI-powered retrieval"

# Metadata Storage
metadata:
  checksums_file: "data/metadata/checksums.json"
